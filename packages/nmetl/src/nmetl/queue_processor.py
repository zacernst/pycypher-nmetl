"""
This module defines the core components for processing items in a queue-based
system within the pycypher library.

It provides an abstract base class, ``QueueProcessor``, and concrete
implementations for specific tasks in the data processing pipeline.

Overall Purpose
---------------

The primary responsibility of this module is to define how data is processed
within the ``Session`` system. The data is received on a queue, and transformed
and passed to another queue.

Key Components
--------------

1.  **QueueProcessor (Abstract Base Class):**

    *   Serves as the foundation for all queue processing operations.
    *   Provides a template for processing items from an incoming queue,
        transforming them, and placing them on an outgoing queue.
    *   Manages thread creation and lifecycle for processing.
    *   Tracks statistics like the number of received and sent items.
    *   Provides a basic error handling mechanism, capturing exceptions during
        processing and putting them on a status queue.
    *   Requires concrete implementations to define the actual processing logic
        in the `process_item_from_queue` method.

2.  **RawDataProcessor (Concrete Class):**

    *   Responsible for handling raw data rows received from ``DataSource`` objects.
    *   Takes each row and uses the ``DataSource`` to generate ``AtomicFact`` objects from it.
    *   Passes the generated facts to the next stage in the pipeline.

3.  **FactGeneratedQueueProcessor (Concrete Class):**

    *   Handles ``AtomicFact`` objects generated by the ``RawDataProcessor``.
    *   Inserts new facts into the ``FactCollection``.
    *   Ensures that facts are only added once to avoid duplication.
    *   Puts the newly-added fact onto the next queue to be checked for triggers.

4. **CheckFactAgainstTriggersQueueProcessor (Concrete Class)**:
    * Reads facts from the check_fact_against_triggers queue and checks them
      against the registered triggers.
    * If a fact matches a trigger's constraints, it creates a `SubTriggerPair`
      and puts it on the outgoing queue.

5. **TriggeredLookupProcessor (Concrete Class)**:
    * Reads sub_trigger_pairs and does the lookup on the fact collection to
      execute a trigger's associated function.
    * Handles running the trigger function, and generating new facts as a result
      of running the trigger function.

Workflow Overview
-----------------

1.  **Data Ingestion:** Raw data is loaded by ``DataSource`` objects and passed
    to the ``raw_input_queue``.
2.  **Raw Data Processing:** The ``RawDataProcessor`` reads from the
``raw_input_queue`` and generates facts.
3.  **Fact Storage:** The ``FactGeneratedQueueProcessor`` adds the generated
facts to the ``FactCollection``.
4. **Check Facts Against Triggers**: The
`CheckFactAgainstTriggersQueueProcessor` checks the facts against the triggers
and puts any matching triggers on the `triggered_lookup_processor_queue`.
5. **Trigger Execution**: The `TriggeredLookupProcessor` executes the triggers
functions, creating new facts.
6. **Repeat**: Steps 3-5 repeat until no more facts are generated.

"""

from __future__ import annotations

import datetime
import queue
import sys
import threading
import traceback
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

from nmetl.helpers import QueueGenerator
from nmetl.trigger import CypherTrigger
from pycypher.fact import AtomicFact, FactNodeHasAttributeWithValue, NullResult
from pycypher.logger import LOGGER
from pycypher.node_classes import (
    AliasedName,
    Collection,
    QueryValueOfNodeAttribute,
)


@dataclass
class SubTriggerPair:
    """A pair of a sub and a trigger."""

    sub: Dict[str, str]
    trigger: CypherTrigger

    def __hash__(self):
        return hash(
            (
                tuple(self.sub),
                self.trigger,
            )
        )


class QueueProcessor(ABC):  # pylint: disable=too-few-public-methods,too-many-instance-attributes
    """ABC that processes items from a queue and places the results onto another queue."""

    def __init__(
        self,
        session: Optional["Session"] = None,
        incoming_queue: Optional[QueueGenerator] = None,
        outgoing_queue: Optional[QueueGenerator] = None,
        status_queue: Optional[queue.Queue] = None,
    ) -> None:
        self.session = session
        self.processing_thread = threading.Thread(
            target=self.process_queue, name=self.__class__.__name__
        )
        self.started = False
        self.started_at = None
        self.finished = False
        self.finished_at = None
        self.received_counter = 0
        self.sent_counter = 0
        self.incoming_queue = incoming_queue
        self.outgoing_queue = outgoing_queue
        self.status_queue = status_queue

        if self.outgoing_queue:
            self.outgoing_queue.incoming_queue_processors.append(self)

    def process_queue(self) -> None:
        """Process every item in the queue using the yield_items method."""
        self.started = True
        self.started_at = datetime.datetime.now()
        for item in self.incoming_queue.yield_items():
            self.received_counter += 1
            try:
                out = self._process_item_from_queue(item)
            except Exception as e:  # pylint: disable=broad-except
                exc_type, exc_value, exc_traceback = sys.exc_info()
                formatted_traceback = traceback.format_exception(
                    exc_type, exc_value, exc_traceback
                )
                formatted_traceback = "\n".join(
                    [line.strip() for line in formatted_traceback]
                )
                error_msg = f"in thread: {threading.current_thread().name}\n"
                error_msg += f"Error processing item {item}: {e}]\n"
                error_msg += f"Traceback: {formatted_traceback}]\n"
                LOGGER.error(error_msg)
                self.status_queue.put(e)
                continue
            if not out:
                continue
            if not isinstance(out, list):
                out = [out]
            for out_item in out:
                self.outgoing_queue.put(out_item)
                self.sent_counter += 1
        self.finished = True
        self.finished_at = datetime.datetime.now()

    @abstractmethod
    def process_item_from_queue(self, item: Any) -> Any:
        """Process an item from the queue."""

    def _process_item_from_queue(self, item: Any) -> Any:
        """Wrap the process call in case we want some logging."""
        return self.process_item_from_queue(item)


class RawDataProcessor(QueueProcessor):
    """Runs in a thread to process raw data from all the DataSource objects."""

    def process_item_from_queue(self, item) -> List[AtomicFact]:
        """Process raw data from the ``raw_input_queue``, generate facts."""
        data_source = item.data_source
        row = item.row
        out = []
        for fact in data_source.generate_raw_facts_from_row(row):
            out.append(fact)
        return out


class FactGeneratedQueueProcessor(QueueProcessor):  # pylint: disable=too-few-public-methods
    """
    Reads from the fact_generated_queue and processes the facts
    by inserting them into the ``FactCollection``.
    """

    def process_item_from_queue(self, item: Any) -> None:
        """Process new facts from the fact_generated_queue."""
        if item in self.session.fact_collection:
            LOGGER.debug("Fact %s already in collection", item)
            return
        item.session = self.session
        self.session.fact_collection.append(item)
        # Put the fact in the queue to be checked for triggers
        self.outgoing_queue.put(item)


class CheckFactAgainstTriggersQueueProcessor(QueueProcessor):  # pylint: disable=too-few-public-methods
    """
    Reads from the check_fact_against_triggers_queue and processes the facts
    by checking them against the triggers.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def process_item_from_queue(self, item: Any) -> None:
        """Process new facts from the check_fact_against_triggers_queue."""

        out = []
        LOGGER.debug("Checking fact %s against triggers", item)
        for _, trigger in self.session.trigger_dict.items():
            LOGGER.debug("Checking trigger %s", trigger)
            for constraint in trigger.constraints:
                LOGGER.debug(
                    "Checking item: %s, constraint %s, trigger %s result: %s",
                    item,
                    constraint,
                    trigger,
                    item + constraint,
                )

                if sub := item + constraint:
                    LOGGER.debug("Fact %s matched a trigger", item)
                    sub_trigger_pair = SubTriggerPair(sub=sub, trigger=trigger)
                    out.append(sub_trigger_pair)
        return out


class TriggeredLookupProcessor(QueueProcessor):  # pylint: disable=too-few-public-methods
    """
    Reads from the check_fact_against_triggers_queue and processes the facts
    by checking them against the triggers.
    """

    WITH_CLAUSE_PROJECTION_KEY = "__with_clause_projection__"
    MATCH_SOLUTION_KEY = "__match_solution__"

    def process_item_from_queue(
        self, item: SubTriggerPair
    ) -> List[Any] | None:
        """Process new facts from the check_fact_against_triggers_queue."""
        self.started = True
        self.started_at = datetime.datetime.now()
        self.received_counter += 1

        try:
            return self._process_sub_trigger_pair(item)
        except Exception as e:  # pylint: disable=broad-exception-caught
            LOGGER.error("Error processing trigger: %s", e)
            self.status_queue.put(e)
        finally:
            self.finished = True
            self.finished_at = datetime.datetime.now()
            return None

    def _process_sub_trigger_pair(
        self, sub_trigger_pair: SubTriggerPair
    ) -> List[Any]:
        """Helper function to process a sub_trigger_pair"""
        variable_to_set = sub_trigger_pair.trigger.variable_set
        fact_collection = self.session.fact_collection
        return_clause = (
            sub_trigger_pair.trigger.cypher.parse_tree.cypher.return_clause
        )
        solutions = return_clause._evaluate(fact_collection)

        computed_facts = []
        for solution in solutions:
            try:
                computed_facts.append(
                    self._process_solution(
                        solution,
                        sub_trigger_pair,
                        variable_to_set,
                        return_clause,
                    )
                )
            except Exception as e:
                LOGGER.error("Error processing solution: %s", e)
                self.status_queue.put(e)

        return computed_facts

    def _process_solution(
        self,
        solution: Dict,
        sub_trigger_pair: SubTriggerPair,
        variable_to_set: str,
        return_clause,
    ) -> FactNodeHasAttributeWithValue:
        """Process a solution and generate a fact."""
        splat = self._extract_splat_from_solution(solution, return_clause)
        if any(isinstance(arg, NullResult) for arg in splat):
            LOGGER.debug("NullResult found in splat %s", splat)
            return NullResult

        computed_value = sub_trigger_pair.trigger.function(*splat)
        sub_trigger_pair.trigger.call_counter += 1
        target_attribute = sub_trigger_pair.trigger.attribute_set
        node_id = self._extract_node_id_from_solution(
            solution, variable_to_set
        )

        computed_fact = FactNodeHasAttributeWithValue(
            node_id=node_id, attribute=target_attribute, value=computed_value
        )
        LOGGER.debug(">>>>>>> Computed fact: %s", computed_fact)
        self.session.fact_generated_queue.put(computed_fact)
        return computed_fact

    def _extract_splat_from_solution(
        self, solution: Dict, return_clause
    ) -> List[Any]:
        """Extract the splat (arguments for the trigger function) from a solution."""

        def to_python(x):
            if isinstance(x, Collection):
                return [to_python(y) for y in x.values]
            return x

        try:
            return [
                to_python(solution.get(alias.name))
                for alias in return_clause.projection.lookups
            ]
        except Exception as e:
            raise ValueError(f"Error extracting splat: {e}") from e

    def _extract_node_id_from_solution(
        self, solution: Dict, variable_to_set: str
    ) -> str:
        """Extract the node ID from the solution."""
        try:
            node_id = solution[self.WITH_CLAUSE_PROJECTION_KEY][
                self.MATCH_SOLUTION_KEY
            ][variable_to_set]
            return node_id
        except KeyError as e:
            raise ValueError(f"Error extracting node ID: {e}") from e

    def original_process_item_from_queue(
        self, item: SubTriggerPair
    ) -> List[Any]:
        """Process new facts from the check_fact_against_triggers_queue."""
        sub_trigger_obj = item
        self.started = True
        self.started_at = datetime.datetime.now()
        self.received_counter += 1
        variable_to_set = sub_trigger_obj.trigger.variable_set
        # match_clause = (
        #     sub_trigger_obj.trigger.cypher.parse_tree.cypher.match_clause
        # )
        # # match_clause.constraints.append(specific_object_constraint)
        fact_collection = self.session.fact_collection

        return_clause = (
            sub_trigger_obj.trigger.cypher.parse_tree.cypher.return_clause
        )
        solutions = return_clause._evaluate(fact_collection)  # pylint: disable=protected-access

        def to_python(x):
            if isinstance(x, Collection):
                return [to_python(y) for y in x.values]
            return x

        for solution in solutions:
            splat = [
                to_python(solution.get(alias.name))
                for alias in return_clause.projection.lookups
            ]
            if any(isinstance(arg, NullResult) for arg in splat):
                LOGGER.debug("NullResult found in splat %s", splat)
                continue
            # Prevent call from happening if NullResult is present
            computed_value = sub_trigger_obj.trigger.function(*splat)
            sub_trigger_obj.trigger.call_counter += 1
            target_attribute = sub_trigger_obj.trigger.attribute_set
            # variable no longer present in solution because of alias renaming
            # import pdb; pdb.set_trace()
            # node_id = alias[name]
            node_id = solution["__with_clause_projection__"][
                "__match_solution__"
            ][variable_to_set]
            computed_fact = FactNodeHasAttributeWithValue(
                node_id=node_id,
                attribute=target_attribute,
                value=computed_value,
            )
            LOGGER.debug(">>>>>>> Computed fact: %s", computed_fact)
            self.session.fact_generated_queue.put(computed_fact)
        self.finished = True
        self.finished_at = datetime.datetime.now()

    def process_item_from_queue_bak(self, item: SubTriggerPair) -> List[Any]:
        """Process new facts from the check_fact_against_triggers_queue."""
        sub_trigger_obj = item
        self.started = True
        self.started_at = datetime.datetime.now()
        self.received_counter += 1
        variable = tuple(sub_trigger_obj.sub)[0]
        node_id = sub_trigger_obj.sub[variable]  # pylint: disable=unused-variable
        match_clause = (
            sub_trigger_obj.trigger.cypher.parse_tree.cypher.match_clause
        )
        # match_clause.constraints.append(specific_object_constraint)
        fact_collection = self.session.fact_collection

        solutions = match_clause.solutions(fact_collection)  # pylint: disable=unused-variable
        return_clause = (
            sub_trigger_obj.trigger.cypher.parse_tree.cypher.return_clause
        )
        aliases = return_clause.projection.lookups
        for solution in solutions:
            splat = []
            for alias in aliases:
                if hasattr(alias, "reference") and hasattr(
                    alias.reference, "aggregation"
                ):  # pylint: disable=no-else-raise
                    # TODO: Implement aggregation in RETURN statement
                    raise NotImplementedError(
                        "Aggregation in RETURN not yet implemented"
                    )
                elif isinstance(alias, AliasedName):
                    variable = alias.name
                    node_id = solution[variable]
                    attribute_value_query = QueryValueOfNodeAttribute(
                        node_id=node_id,
                        attribute=alias.name,
                    )
                else:
                    variable = alias.reference.object  # HERE
                    node_id = solution[variable]
                    attribute = alias.reference.attribute
                    attribute_value_query = QueryValueOfNodeAttribute(
                        node_id=node_id,
                        attribute=attribute,
                    )
                attribute_value = fact_collection.query(attribute_value_query)
                splat.append(attribute_value)
            if any(isinstance(arg, NullResult) for arg in splat):
                continue
            computed_value = sub_trigger_obj.trigger.function(*splat)
            sub_trigger_obj.trigger.call_counter += 1
            target_attribute = sub_trigger_obj.trigger.attribute_set
            computed_fact = FactNodeHasAttributeWithValue(
                node_id=node_id,
                attribute=target_attribute,
                value=computed_value,
            )
            self.session.fact_generated_queue.put(computed_fact)
        self.finished = True
        self.finished_at = datetime.datetime.now()
